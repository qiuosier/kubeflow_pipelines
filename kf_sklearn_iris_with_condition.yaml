apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: sklearn-iris-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0, pipelines.kubeflow.org/pipeline_compilation_time: '2021-03-26T15:35:23.668878',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "Classifying Iris data
      with KNN.", "name": "sklearn-iris"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.4.0}
spec:
  entrypoint: sklearn-iris
  templates:
  - name: condition-1
    inputs:
      artifacts:
      - {name: split-data-test_x}
      - {name: split-data-test_y}
      - {name: split-data-train_x}
      - {name: split-data-train_y}
    dag:
      tasks:
      - name: test-model
        template: test-model
        dependencies: [train-knn]
        arguments:
          artifacts:
          - {name: split-data-test_x, from: '{{inputs.artifacts.split-data-test_x}}'}
          - {name: split-data-test_y, from: '{{inputs.artifacts.split-data-test_y}}'}
          - {name: train-knn-model, from: '{{tasks.train-knn.outputs.artifacts.train-knn-model}}'}
      - name: train-knn
        template: train-knn
        arguments:
          artifacts:
          - {name: split-data-train_x, from: '{{inputs.artifacts.split-data-train_x}}'}
          - {name: split-data-train_y, from: '{{inputs.artifacts.split-data-train_y}}'}
  - name: condition-2
    inputs:
      artifacts:
      - {name: split-data-test_x}
      - {name: split-data-test_y}
      - {name: split-data-train_x}
      - {name: split-data-train_y}
    dag:
      tasks:
      - name: test-model-2
        template: test-model-2
        dependencies: [train-logistics]
        arguments:
          artifacts:
          - {name: split-data-test_x, from: '{{inputs.artifacts.split-data-test_x}}'}
          - {name: split-data-test_y, from: '{{inputs.artifacts.split-data-test_y}}'}
          - {name: train-logistics-model, from: '{{tasks.train-logistics.outputs.artifacts.train-logistics-model}}'}
      - name: train-logistics
        template: train-logistics
        arguments:
          artifacts:
          - {name: split-data-train_x, from: '{{inputs.artifacts.split-data-train_x}}'}
          - {name: split-data-train_y, from: '{{inputs.artifacts.split-data-train_y}}'}
  - name: load-data
    container:
      args: [--x, /tmp/outputs/x/data, --y, /tmp/outputs/y/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_data(x_path, y_path):
            import numpy
            from sklearn import datasets
            iris_x, iris_y = datasets.load_iris(return_X_y=True)
            with open(x_path, 'wb') as f:
                numpy.save(f, iris_x)
            with open(y_path, 'wb') as f:
                numpy.save(f, iris_y)

        import argparse
        _parser = argparse.ArgumentParser(prog='Load data', description='')
        _parser.add_argument("--x", dest="x_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_data(**_parsed_args)
      image: qiuosier/sklearn
    outputs:
      artifacts:
      - {name: load-data-x, path: /tmp/outputs/x/data}
      - {name: load-data-y, path: /tmp/outputs/y/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"outputPath": "x"}, "--y", {"outputPath": "y"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_data(x_path, y_path):\n    import numpy\n    from
          sklearn import datasets\n    iris_x, iris_y = datasets.load_iris(return_X_y=True)\n    with
          open(x_path, ''wb'') as f:\n        numpy.save(f, iris_x)\n    with open(y_path,
          ''wb'') as f:\n        numpy.save(f, iris_y)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Load data'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\", dest=\"y_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = load_data(**_parsed_args)\n"],
          "image": "qiuosier/sklearn"}}, "name": "Load data", "outputs": [{"name":
          "x", "type": "String"}, {"name": "y", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: select-classifier
    container:
      args: [--minimum, '0', --maximum, '9', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def select_classifier(minimum, maximum):
            """Generate a random number between minimum and maximum (inclusive)."""
            import random
            result = random.randint(minimum, maximum)
            print(result)
            return result

        def _serialize_int(int_value: int) -> str:
            if isinstance(int_value, str):
                return int_value
            if not isinstance(int_value, int):
                raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
            return str(int_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Select classifier', description='Generate a random number between minimum and maximum (inclusive).')
        _parser.add_argument("--minimum", dest="minimum", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--maximum", dest="maximum", type=int, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = select_classifier(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_int,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
    outputs:
      parameters:
      - name: select-classifier-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: select-classifier-Output, path: /tmp/outputs/Output/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Generate
          a random number between minimum and maximum (inclusive).", "implementation":
          {"container": {"args": ["--minimum", {"inputValue": "minimum"}, "--maximum",
          {"inputValue": "maximum"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" >
          \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def select_classifier(minimum,
          maximum):\n    \"\"\"Generate a random number between minimum and maximum
          (inclusive).\"\"\"\n    import random\n    result = random.randint(minimum,
          maximum)\n    print(result)\n    return result\n\ndef _serialize_int(int_value:
          int) -> str:\n    if isinstance(int_value, str):\n        return int_value\n    if
          not isinstance(int_value, int):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of int.''.format(str(int_value), str(type(int_value))))\n    return
          str(int_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Select
          classifier'', description=''Generate a random number between minimum and
          maximum (inclusive).'')\n_parser.add_argument(\"--minimum\", dest=\"minimum\",
          type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--maximum\",
          dest=\"maximum\", type=int, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = select_classifier(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_int,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "minimum", "type": "Integer"},
          {"name": "maximum", "type": "Integer"}], "name": "Select classifier", "outputs":
          [{"name": "Output", "type": "Integer"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"maximum": "9", "minimum":
          "0"}', pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: sklearn-iris
    dag:
      tasks:
      - name: condition-1
        template: condition-1
        when: '{{tasks.select-classifier.outputs.parameters.select-classifier-Output}}
          >= 5'
        dependencies: [select-classifier, split-data]
        arguments:
          artifacts:
          - {name: split-data-test_x, from: '{{tasks.split-data.outputs.artifacts.split-data-test_x}}'}
          - {name: split-data-test_y, from: '{{tasks.split-data.outputs.artifacts.split-data-test_y}}'}
          - {name: split-data-train_x, from: '{{tasks.split-data.outputs.artifacts.split-data-train_x}}'}
          - {name: split-data-train_y, from: '{{tasks.split-data.outputs.artifacts.split-data-train_y}}'}
      - name: condition-2
        template: condition-2
        when: '{{tasks.select-classifier.outputs.parameters.select-classifier-Output}}
          < 5'
        dependencies: [select-classifier, split-data]
        arguments:
          artifacts:
          - {name: split-data-test_x, from: '{{tasks.split-data.outputs.artifacts.split-data-test_x}}'}
          - {name: split-data-test_y, from: '{{tasks.split-data.outputs.artifacts.split-data-test_y}}'}
          - {name: split-data-train_x, from: '{{tasks.split-data.outputs.artifacts.split-data-train_x}}'}
          - {name: split-data-train_y, from: '{{tasks.split-data.outputs.artifacts.split-data-train_y}}'}
      - {name: load-data, template: load-data}
      - {name: select-classifier, template: select-classifier}
      - name: split-data
        template: split-data
        dependencies: [load-data]
        arguments:
          artifacts:
          - {name: load-data-x, from: '{{tasks.load-data.outputs.artifacts.load-data-x}}'}
          - {name: load-data-y, from: '{{tasks.load-data.outputs.artifacts.load-data-y}}'}
  - name: split-data
    container:
      args: [--x, /tmp/inputs/x/data, --y, /tmp/inputs/y/data, --train-x, /tmp/outputs/train_x/data,
        --test-x, /tmp/outputs/test_x/data, --train-y, /tmp/outputs/train_y/data,
        --test-y, /tmp/outputs/test_y/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef split_data(\n    x_path, \n    y_path,\n    train_x_path, \n    test_x_path,\n\
        \    train_y_path, \n    test_y_path,\n):\n    import numpy\n    from sklearn.model_selection\
        \ import train_test_split\n    iris_x = numpy.load(x_path)\n    iris_y = numpy.load(y_path)\n\
        \    train_x, test_x, train_y , test_y = train_test_split(iris_x, iris_y,\
        \ test_size=0.2)\n    print(f\"Training data size - X: {train_x.size}, y:\
        \ {train_y.size}\")\n    print(f\"Testing data size - X: {test_x.size}, y:\
        \ {test_y.size}\")\n\n    with open(train_x_path, 'wb') as f:\n        numpy.save(f,\
        \ train_x)\n    with open(test_x_path, 'wb') as f:\n        numpy.save(f,\
        \ test_x)\n    with open(train_y_path, 'wb') as f:\n        numpy.save(f,\
        \ train_y)\n    with open(test_y_path, 'wb') as f:\n        numpy.save(f,\
        \ test_y)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Split\
        \ data', description='')\n_parser.add_argument(\"--x\", dest=\"x_path\", type=str,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --train-x\", dest=\"train_x_path\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-x\"\
        , dest=\"test_x_path\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-y\", dest=\"\
        train_y_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--test-y\", dest=\"test_y_path\", type=_make_parent_dirs_and_return_path,\
        \ required=True, default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\
        \n_outputs = split_data(**_parsed_args)\n"
      image: qiuosier/sklearn
    inputs:
      artifacts:
      - {name: load-data-x, path: /tmp/inputs/x/data}
      - {name: load-data-y, path: /tmp/inputs/y/data}
    outputs:
      artifacts:
      - {name: split-data-test_x, path: /tmp/outputs/test_x/data}
      - {name: split-data-test_y, path: /tmp/outputs/test_y/data}
      - {name: split-data-train_x, path: /tmp/outputs/train_x/data}
      - {name: split-data-train_y, path: /tmp/outputs/train_y/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputPath": "x"}, "--y", {"inputPath": "y"}, "--train-x",
          {"outputPath": "train_x"}, "--test-x", {"outputPath": "test_x"}, "--train-y",
          {"outputPath": "train_y"}, "--test-y", {"outputPath": "test_y"}], "command":
          ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef split_data(\n    x_path, \n    y_path,\n    train_x_path,
          \n    test_x_path,\n    train_y_path, \n    test_y_path,\n):\n    import
          numpy\n    from sklearn.model_selection import train_test_split\n    iris_x
          = numpy.load(x_path)\n    iris_y = numpy.load(y_path)\n    train_x, test_x,
          train_y , test_y = train_test_split(iris_x, iris_y, test_size=0.2)\n    print(f\"Training
          data size - X: {train_x.size}, y: {train_y.size}\")\n    print(f\"Testing
          data size - X: {test_x.size}, y: {test_y.size}\")\n\n    with open(train_x_path,
          ''wb'') as f:\n        numpy.save(f, train_x)\n    with open(test_x_path,
          ''wb'') as f:\n        numpy.save(f, test_x)\n    with open(train_y_path,
          ''wb'') as f:\n        numpy.save(f, train_y)\n    with open(test_y_path,
          ''wb'') as f:\n        numpy.save(f, test_y)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Split data'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-x\",
          dest=\"train_x_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-x\", dest=\"test_x_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-y\",
          dest=\"train_y_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-y\", dest=\"test_y_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = split_data(**_parsed_args)\n"],
          "image": "qiuosier/sklearn"}}, "inputs": [{"name": "x"}, {"name": "y"}],
          "name": "Split data", "outputs": [{"name": "train_x", "type": "String"},
          {"name": "test_x", "type": "String"}, {"name": "train_y", "type": "String"},
          {"name": "test_y", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
  - name: test-model
    container:
      args: [--test-x, /tmp/inputs/test_x/data, --test-y, /tmp/inputs/test_y/data,
        --model, /tmp/inputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def test_model(test_x_path, test_y_path, model_path):
            import numpy
            import pickle
            from sklearn.metrics import classification_report
            test_x = numpy.load(test_x_path)
            test_y = numpy.load(test_y_path)
            print(f"Testing data size - X: {test_x.size}, y: {test_y.size}")
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            pred_y = model.predict(test_x)
            print(classification_report(test_y, pred_y))

        import argparse
        _parser = argparse.ArgumentParser(prog='Test model', description='')
        _parser.add_argument("--test-x", dest="test_x_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-y", dest="test_y_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = test_model(**_parsed_args)
      image: qiuosier/sklearn
    inputs:
      artifacts:
      - {name: train-knn-model, path: /tmp/inputs/model/data}
      - {name: split-data-test_x, path: /tmp/inputs/test_x/data}
      - {name: split-data-test_y, path: /tmp/inputs/test_y/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--test-x", {"inputPath": "test_x"}, "--test-y", {"inputPath":
          "test_y"}, "--model", {"inputPath": "model"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def test_model(test_x_path, test_y_path,
          model_path):\n    import numpy\n    import pickle\n    from sklearn.metrics
          import classification_report\n    test_x = numpy.load(test_x_path)\n    test_y
          = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size},
          y: {test_y.size}\")\n    with open(model_path, ''rb'') as f:\n        model
          = pickle.load(f)\n    pred_y = model.predict(test_x)\n    print(classification_report(test_y,
          pred_y))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--test-x\", dest=\"test_x_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-y\",
          dest=\"test_y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_model(**_parsed_args)\n"],
          "image": "qiuosier/sklearn"}}, "inputs": [{"name": "test_x"}, {"name": "test_y"},
          {"name": "model"}], "name": "Test model"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: test-model-2
    container:
      args: [--test-x, /tmp/inputs/test_x/data, --test-y, /tmp/inputs/test_y/data,
        --model, /tmp/inputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def test_model(test_x_path, test_y_path, model_path):
            import numpy
            import pickle
            from sklearn.metrics import classification_report
            test_x = numpy.load(test_x_path)
            test_y = numpy.load(test_y_path)
            print(f"Testing data size - X: {test_x.size}, y: {test_y.size}")
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
            pred_y = model.predict(test_x)
            print(classification_report(test_y, pred_y))

        import argparse
        _parser = argparse.ArgumentParser(prog='Test model', description='')
        _parser.add_argument("--test-x", dest="test_x_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--test-y", dest="test_y_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = test_model(**_parsed_args)
      image: qiuosier/sklearn
    inputs:
      artifacts:
      - {name: train-logistics-model, path: /tmp/inputs/model/data}
      - {name: split-data-test_x, path: /tmp/inputs/test_x/data}
      - {name: split-data-test_y, path: /tmp/inputs/test_y/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--test-x", {"inputPath": "test_x"}, "--test-y", {"inputPath":
          "test_y"}, "--model", {"inputPath": "model"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def test_model(test_x_path, test_y_path,
          model_path):\n    import numpy\n    import pickle\n    from sklearn.metrics
          import classification_report\n    test_x = numpy.load(test_x_path)\n    test_y
          = numpy.load(test_y_path)\n    print(f\"Testing data size - X: {test_x.size},
          y: {test_y.size}\")\n    with open(model_path, ''rb'') as f:\n        model
          = pickle.load(f)\n    pred_y = model.predict(test_x)\n    print(classification_report(test_y,
          pred_y))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Test
          model'', description='''')\n_parser.add_argument(\"--test-x\", dest=\"test_x_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--test-y\",
          dest=\"test_y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = test_model(**_parsed_args)\n"],
          "image": "qiuosier/sklearn"}}, "inputs": [{"name": "test_x"}, {"name": "test_y"},
          {"name": "model"}], "name": "Test model"}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-knn
    container:
      args: [--train-x, /tmp/inputs/train_x/data, --train-y, /tmp/inputs/train_y/data,
        --model, /tmp/outputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_knn(train_x_path, train_y_path, model_path):
            import numpy
            import pickle
            from sklearn.neighbors import KNeighborsClassifier
            train_x = numpy.load(train_x_path)
            train_y = numpy.load(train_y_path)
            print(f"Training data size - X: {train_x.size}, y: {train_y.size}")
            knn = KNeighborsClassifier()
            knn.fit(train_x, train_y)
            with open(model_path, 'wb') as f:
                pickle.dump(knn, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train knn', description='')
        _parser.add_argument("--train-x", dest="train_x_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-y", dest="train_y_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_knn(**_parsed_args)
      image: qiuosier/sklearn
    inputs:
      artifacts:
      - {name: split-data-train_x, path: /tmp/inputs/train_x/data}
      - {name: split-data-train_y, path: /tmp/inputs/train_y/data}
    outputs:
      artifacts:
      - {name: train-knn-model, path: /tmp/outputs/model/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-x", {"inputPath": "train_x"}, "--train-y", {"inputPath":
          "train_y"}, "--model", {"outputPath": "model"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_knn(train_x_path, train_y_path, model_path):\n    import
          numpy\n    import pickle\n    from sklearn.neighbors import KNeighborsClassifier\n    train_x
          = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training
          data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = KNeighborsClassifier()\n    knn.fit(train_x,
          train_y)\n    with open(model_path, ''wb'') as f:\n        pickle.dump(knn,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train knn'',
          description='''')\n_parser.add_argument(\"--train-x\", dest=\"train_x_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-y\",
          dest=\"train_y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_knn(**_parsed_args)\n"], "image": "qiuosier/sklearn"}}, "inputs":
          [{"name": "train_x"}, {"name": "train_y"}], "name": "Train knn", "outputs":
          [{"name": "model"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: train-logistics
    container:
      args: [--train-x, /tmp/inputs/train_x/data, --train-y, /tmp/inputs/train_y/data,
        --model, /tmp/outputs/model/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def train_logistics(train_x_path, train_y_path, model_path):
            import numpy
            import pickle
            from sklearn.linear_model import LogisticRegression
            train_x = numpy.load(train_x_path)
            train_y = numpy.load(train_y_path)
            print(f"Training data size - X: {train_x.size}, y: {train_y.size}")
            knn = LogisticRegression()
            knn.fit(train_x, train_y)
            with open(model_path, 'wb') as f:
                pickle.dump(knn, f)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train logistics', description='')
        _parser.add_argument("--train-x", dest="train_x_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--train-y", dest="train_y_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_logistics(**_parsed_args)
      image: qiuosier/sklearn
    inputs:
      artifacts:
      - {name: split-data-train_x, path: /tmp/inputs/train_x/data}
      - {name: split-data-train_y, path: /tmp/inputs/train_y/data}
    outputs:
      artifacts:
      - {name: train-logistics-model, path: /tmp/outputs/model/data}
    metadata:
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train-x", {"inputPath": "train_x"}, "--train-y", {"inputPath":
          "train_y"}, "--model", {"outputPath": "model"}], "command": ["sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef train_logistics(train_x_path, train_y_path, model_path):\n    import
          numpy\n    import pickle\n    from sklearn.linear_model import LogisticRegression\n    train_x
          = numpy.load(train_x_path)\n    train_y = numpy.load(train_y_path)\n    print(f\"Training
          data size - X: {train_x.size}, y: {train_y.size}\")\n    knn = LogisticRegression()\n    knn.fit(train_x,
          train_y)\n    with open(model_path, ''wb'') as f:\n        pickle.dump(knn,
          f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train logistics'',
          description='''')\n_parser.add_argument(\"--train-x\", dest=\"train_x_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--train-y\",
          dest=\"train_y_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model\",
          dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_logistics(**_parsed_args)\n"], "image": "qiuosier/sklearn"}}, "inputs":
          [{"name": "train_x"}, {"name": "train_y"}], "name": "Train logistics", "outputs":
          [{"name": "model"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
